Asymptotic analysis

As an illustration, suppose that we are interested in the properties of a function f (n) as n becomes very large. If f(n) = n2 + 3n, then as n becomes very large, the term 3n becomes insignificant compared to n2. The function f(n) is said to be "asymptotically equivalent to n2, as n → ∞". This is often written symbolically as f (n) ~ n2, which is read as "f(n) is asymptotic to n2".
An example of an important asymptotic result is the prime number theorem. Let π(x) denote the prime-counting function (which is not directly related to the constant pi), i.e. π(x) is the number of prime numbers that are less than or equal to x. Then the theorem states that π ( x ) ∼ x ln ⁡ x . {\displaystyle \pi (x)\sim {\frac {x}{\ln x}}.}
Asymptotic analysis is commonly used in computer science as part of the analysis of algorithms and is often expressed there in terms of big O notation.

hazards

## n is often small

so, O(1) ~ O(n) ~ O(nlogn) ~ etc

maintenance cost, depends, if you have to maintain the code, then maybe O(n) is better, because in may become more cleaner
we have tradeoff between time and space
from system perspective, the space is time
example:

1. Memoization (Time for Space)

   Problem: Computing Fibonacci numbers recursively takes exponential time.

   Without space: fib(n) has time complexity O(2n)O(2n)

   With memoization: Store intermediate results in a table — space used is O(n)O(n), but time becomes O(n)O(n)

You're trading memory (space) to save recomputation (time).

2. Caching (in OS, Web, CPU, etc.)

   Web: Caching frequently accessed content avoids recomputation and repeated downloads.

   CPU: L1/L2 caches store frequently used data, minimizing access time to RAM.

You’re spending more space (memory) to gain speed (less time).

s
